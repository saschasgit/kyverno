= Kyverno - Eine Übersicht

:toc:

== Vorwort

Kyverno ist ein Admission Controller, welcher sehr viele Möglichkeiten gibt, um über Policies Vorgaben auf einem OpenShift-Cluster umzusetzen. Dabei ist Kyverno "Kubernetes-native" und verwendet als Sprache für die Polices YAML, was es für viele einfacher macht, als Polices in Rego zu schreiben.

== Admission Controller

Zunächst kurz ein paar Sätze dazu, was überhaupt "Admission Controller" sind. Immer, wenn ein Objekt in OpenShift angelegt, verändert oder gelöscht wird, läuft diese Anforderung über die API von OpenShift. Und das unabhängig davon, ob man den OC-Client oder die Weboberfläche verwendet.

Und während das Opject über die API läuft, gibt einem OpenShift die Möglichkeit, bei beliebigen Objekten einzugreifen. Die Objekte können verändert und überprüft werden.

https://cloud.redhat.com/blog/dynamic-admission-control

image:pictures/admission-controller.png["Admission Controller"]

Wie man sieht, gibt es eine "Mutating admission" und eine "Validating admission", über die man durch den Webhook eingreifen kann. Und genau diesen Mechanismus nutzen Admission Controller.

Auf einem OpenShift Cluster gibt es unterschiedliche Admission Controller. Ein Beispiel ist der Red Hat Service Mesh, wenn man diesen Operator verwendet. Die Control Plane des Service Meshs prüft beispielsweise über einen eigenen Admission Controller, ob eine bestimmte Annotation in einem Deployment vorhanden ist und fügt dann allen Pods einen Envoy-Proxy als zusätzlichen Container hinzu. Dabei wird die "Mutating admission" verwendet.

Ein Beispiel für die "Validating admission" sind RASP-Tools wie "Red Hat Advanced Cluster Security", "Aquasec" oder "Neuvector". Diese RASP-Tools wollen Objekte nicht verändern, aber überprüfen, ob diese vorgegebenen Anforderungen entsprechen, um die Sicherheit zu gewährleisten.

Kyverno kann beide ansprechen, sowohl "Mutating admission" als auch "Validating admission" und ist damit extrem flexibel einsetzbar.

== Die Kyverno Dokumentation

Die, sehr umfangreiche, Dokumentation zu Kyverno findet man unter https://kyverno.io/docs/

An dieser Stelle kann nur ein erster Überblick bzw. Einstieg gegeben werden. Kyverno ist viel zu umfangreich, um alles hier darzustellen.

== Die Installation von Kyverno

https://kyverno.io/docs/installation/methods/

Kyverno wird normalerweise über Helm installiert. Dies kann man direkt machen, so wie es unter obigem Link beschrieben ist. Bei Bedarf kann man sich den aktuellen Chart lokal herunterladen und verwenden. Dieses Beispiel lädt das Chart in das aktuelle Verzeichnis herunter:

[source]
----
helm pull kyverno/kyverno --untar=true --untardir=.
----

== Die wichtigsten CRDs von Kyverno

Kyverno bringt einige CRDs mit sich. Die wichtigsten sind:

Policy
Eine Policy, welche sich nur auf einen angegebenen Namespace bezieht.

ClusterPolicy
Eine clusterweite Policy

AdmissionReport
Eine Auswertung der Policyanwendungen auf Namespaceabene.

ClusterAdmissionReport
Eine Auswertung der Policyanwendungen auf Clusterebene.

== ClusterPolicy "Check Namespace"

Fangen wir einfach mit einer ersten Policy einmal an. Wir wollen prüfen, ob ein neu ersteller Namespace ein bestimmtes Label enthält. 

Datei: 01-validate-clusterpolicy-check-namespace-label.yaml

[source,yaml]
----
apiVersion: kyverno.io/v2beta1
kind: ClusterPolicy
metadata:
  annotations:
    policies.kyverno.io/description: >-
      Checks if all namespaces have set a label "hello" with the value "world"
    policies.kyverno.io/severity: low
    policies.kyverno.io/subject: Namespace
    policies.kyverno.io/title: Test Namespace Annotation
  name: test-namespace-annotation
spec:
  background: false
  rules:
    - exclude:
        any:
          - resources:
              namespaces:
                - openshift*
                - kube*
                - default
      match:
        any:
          - resources:
              kinds:
                - Namespace
      name: test-namespace-label
      validate:
        anyPattern:
          - metadata:
              label:
                hello: world
        message: All new Namespaces must have a label "hello" with the value "world".
  validationFailureAction: audit
----

Die wichtigsten Elemente der Policy kurz erklärt:

Am "kind" sehen wir, dass es sich natürlich um eine ClusterPolicy handelt. Anders würde das prüfen von Namespaces auch keinen Sinn ergeben.

Im Abschnitt "rules" finden wir die verschiedenen Abschnitte "exclude", "match" und "validate".

Im Abschnitt "exclude" stehen dabei Regeln, auf die diese Policy nicht zutreffen soll. Wir möchten nicht, dass diese Regel auf Namespaces angewendet wird, die mit "openshift" oder "kube" beginnen und sie soll auch nicht für einen Namespace "default" gelten.

Im Abschnitt "match" befindet sich die Regel, worauf die Policy angewendet werden soll. Sie woll für alle Ressoucen vom Typ "Namespace" gelten.

Im Abschnitt "validate" steht, was genau geprüft werden soll. Dass es diesen Abschnitt gibt bedeutet, dass wir hier eine "Validate admission" haben. Geprüft wird, ob das angegebene Label vorhanden ist.

Am Ende steht noch "validationFailureAction: audit". Wir möchten zunächst nur, dass die Policy im "Audit" Modus läuft. Dann wird sie bereits verwendet, aber sie wird noch nicht erzwungen. Ein Namespace kann daher jetzt auch dann noch angelegt werden, wenn das Label nicht vorhanden ist. Die Alternative wäre "validationFailureAction: enforce".

Wenn diese Policy nun aktiv ist, erstellen wir einmal zwei Namespaces:

[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: a-test
  labels:
    hello: world
----

[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: b-test
----

Wenn man die Namespaces anlegt, dann passiert zunächst nichts Besonderes. Dadurch, dass wir eine ClusterPolicy haben, erstellt Kyverno nun aber automatisch einen ClusterAdmissionReport:

[source]
----
oc get clusteradmissionreport -A
NAME                                   AGE   PASS   FAIL   WARN   ERROR   SKIP
55a254a2-eee1-4d29-a6b7-5b1ba890b3be   7s    1      0      0      0       0
839c2835-c9bd-4f41-a7d3-87cd4d8466c1   5s    0      1      0      0       0
----

Und hier sehen wir, dass ein Namespace der Regel entsprach ("PASS") und einer nicht ("FAIL").

Wenn wir in einen Report schauen, sehen wir in der YAML, worauf sich der Report bezieht, also in unserem Fall einen bestimmten Namespace und entweder "result: fail" oder "result: pass".

An dieser Stelle ändern wir nun die Policy. Wir löschen nun einmal die beiden Namespaces und verwenden diese Policy, die genau das gleiche macht, nun aber als Action nicht mehr "audit", sondern "enforce" hat:

Datei: 02-validate-clusterpolicy-check-namespace-label.yaml

[source,yaml]
----
apiVersion: kyverno.io/v2beta1
kind: ClusterPolicy
metadata:
  annotations:
    policies.kyverno.io/description: >-
      Checks if all namespaces have set a label "hello" with the value "world"
    policies.kyverno.io/severity: low
    policies.kyverno.io/subject: Namespace
    policies.kyverno.io/title: Test Namespace Annotation
  name: test-namespace-annotation
spec:
  background: false
  rules:
    - exclude:
        any:
          - resources:
              namespaces:
                - openshift*
                - kube*
                - default
      match:
        any:
          - resources:
              kinds:
                - Namespace
      name: test-namespace-label
      validate:
        anyPattern:
          - metadata:
              label:
                hello: world
        message: All new Namespaces must have a label "hello" with the value "world".
  validationFailureAction: enforce
----



CAUTION: Das Problem lauert aber an einer anderen Stelle. Wenn wir nur eine GitOps-Instanz haben, dann haben wir auch nur einen dahinterliegenden GitOps-Serviceaccount mit den entsprechenden Berechtigungen in dem Namespace. Und wenn ein Developerteam GitOps verwendet, dann werden die Ressourcen nicht mit den Berechtigungen des Developers, sondern mit den Berechtigungen des GitOps-Serviceaccounts erstellt. Hat also der Clusteradmin die Möglichkeit, über GitOps eine Networkpolicy auszurollen, dann hat der Developer das in seinem Namespace ebenso.

Nun kann man dies noch über die Argo-Applikationsprojekte lösen, denn dort lässt sich konfigurieren, welche Art von Kubernetesobjekten wie z.B. "NetworkPolicy" erstellt werden dürfen und welche nicht. Ebenso lässt sich konfigurieren, in welchen Namespaces die Berechtigungen gelten. Diese Konfiguration muss aber sehr bedacht vorgenommen werden, damit am Ende die Berechtigungen korrekt sind.

Wenn man jedoch unterschiedliche Instanzen von Red Hat GitOps einsetzen möchte oder dies gar eine Securityanforderung ist, muss man anders vorgehen. Hierfür sind die beiden folgenden Optionen gedacht.

== Option 2: Getrennte Instanzen und Konfiguration der Instanzberechtigungen

Wenn man getrennte Instanzen von Red Hat GitOps einsetzt, stösst man auf ein Problem. Über ein Label "argocd.argoproj.io/managed-by:" wird in einem Namespace angegeben, welche Instanz diesen Namespace verwaltet. Daher hat erst einmal nur eine Instanz von Red Hat GitOps die Berechtigung, auf den Namespace zuzugreifen.

NOTE: Diese Option habe ich bisher nicht getestet, es dürfte aber keine technischen Probleme geben.

Wenn man nun beide Instanzen berechtigen möchte, auf einen Namespace zuzugreifen, muss man wissen, was hinter den Kulissen passiert, wenn ein Namespace ein entsprechendes Label erhält.

image:pictures/berechtigungsprozess3.png["Berechtigungsprozess"]

Wenn ein Namespace mit dem entsprechenden Label erstellt wird, bermerkt GitOps dies und erstellt im Normalfall eine Rolle und ein Rolebinding für den Serviceaccount des ArgoCD-Application-Controllers. Diese beiden Ressourcen sind an den Namespace gebunden, so dass GitOps darüber Zugriff auf den Namespace erhält.

Diese Rolle und das Rolebinding können wir natürlich auch manuell erstellen, also mit GitOps ausrollen, um die gleichen Berechtigungen wie durch das Label zu erhalten. So haben wir die Möglichkeit, dass die GitOps-Instanz für die Clusteradministratoren über diesen manuellen Weg und die GitOps-Instanz der Developer über das Label im Namespace Berechtigungen auf den Namespace erhalten. (Das geht natürlich grundsätzlich auch anders herum.)

image:pictures/instanzberechtigungen2.png["Instanzberechtigungen"]

CAUTION: Auch hier muss man aufpassen. Das Problem ist die GitOps-Instanz für die Developer. Denn der Serviceaccount, der für die GitOps-Instanz Ressourcen ausrollt, hat zunächst einmal die sehr weitgehende Rechte des Namespace-Admins und damit beispielsweise die Rechte zum Erstellen, Ändern oder Löschen von Subscriptions oder NetworkPolicies. Daraus folgt, dass man dafür sorgen muss, dass dass der Serviceaccount nicht diese weitgehenden Berechtigungen erhält.

Dieses Problem hat man auch im ArgoCD Projekt erkannt (vor ca. einem Jahr war das noch ein größeres Problem) und dafür eine Option erschaffen. Es gibt eine Umgebungsvariable "CONTROLLER_CLUSTER_ROLE", welche auf die Rolle verweist, welche GitOps für den Application-Controller verwendet. Diese kann in der Subscription gesetzt werden. Siehe dazu auch:

https://argocd-operator.readthedocs.io/en/latest/usage/custom_roles/

https://docs.openshift.com/container-platform/4.10/cicd/gitops/gitops-release-notes.html#new-features-1-4-0_gitops-release-notes

https://issues.redhat.com/browse/GITOPS-1290

IMPORTANT: Ich habe noch nicht getestet, ob es eine einfach Option gibt, dieses Verhalten je nach Instanz zu steuern. Das Setting bezieht sich auf die Subscription, also den Operator und die Instanzen erben dies vom Operator. Red Hat schreibt zwar (siehe Issue Link) "Cluster admin can choose to maintain the existing behaviour and request Argo CD to be namespace-admin", bisher ist mir aber noch nicht klar, wie dies einfach gesetzt werden kann.

== Option 3: Getrennte Instanzen und Einsatz des Namespace Configuration Operators

Eine Möglichkeit, dies eleganter zu lösen, so dass man am Ende gar nicht zwei verschiedenen GitOps-Instanzen Berechtigungen auf den Namespace geben muss, ist der Einsatz des "Namespace Configuration Operators". Mit diesem lassen sich sowohl die Namespaces als auch die gewünschte Konfiguration innerhalb dieser Namespaces konfigurieren und über GitOps ausrollen.

image:pictures/namespaceconfigurator.png["Namespace Configuration Operator"]

Der Operator bringt drei CRDs mit sich:

.CRDs
* NamespaceConfig
* GroupConfig
* UserConfig

Die Namen dieser drei CRDs beziehen sich nicht darauf, was sie konfigurieren, sondern durch welches Objekt sie getriggert werden, beispielsweise reagiert eine NamespaceConfig, wenn ein Namespace erstellt wird und nimmt dann eine Konfiguration vor. Und mit einer UserConfig kann man beispielsweise automatisch bei der Anlage eines Users diesem einen Namespace mit einer vorgegebenen Konfiguration bereitstellen.

An dieser Stelle wird nur eine einfache Konfiguration vorgenommen. Wer detaillierter sehen möchte, was der Operator für Möglichkeiten bietet, kann sich folgende Links ansehen:

https://github.com/redhat-cop/namespace-configuration-operator

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/namespace-config/readme.md

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/user-sandbox/readme.md

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/team-onboarding/group-config.yaml

In unserem Beispiel sollen bei der Anlage einer Gruppe (was natürlich über ArgoCD geschehen kann) drei Namespaces angelegt werden und verschiedene RessourceQuotas gesetzt werden. Bei einem Namespace werden zudem LimitRanges angelegt.

NOTE: Dies ist nur ein einfaches Beispiel. Man kann innerhalb der Namespaces beliebige Ressourcen erstellen, beispielsweise NetworkPolicies, etc.

Zunächst legen wir in dem Beispiel zwei Gruppen mit jeweils zwei Usern an. Jedoch wird nur eine der beiden Gruppen, die erste, mit einem Label versehen: "team: important-project"

[source,yaml]
----
kind: Group
apiVersion: user.openshift.io/v1
metadata:
  name: namespace-test-group
  labels:
    team: important-project
users:
  - namespace-testuser1
  - namespace-testuser2
---
kind: Group
apiVersion: user.openshift.io/v1
metadata:
  name: namespace-test-group2
users:
  - namespace-testuser3
  - namespace-testuser4 
----

Nun legen wir eine GroupConfig an. Diese hat einen Labelselector, der zum Label unser ersten Gruppe passt. Damit reagiert die GroupConfig nur auf die Erstellung unser ersten Gruppe, bei der zweiten bleibt der Operator unttätig.

Durch diese GroupConfig werden durch die Erstellung der ersten Gruppe automatisch drei Namespaces angelegt und noch eine beliebge Annotation hinzugefügt. Zudem erhält jeder Namespace ein eigenes Label mit seinem Namen: "namespace: important-project-dev", etc.

[source,yaml]
----
kind: GroupConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-groupconfig
spec:
  labelSelector:
    matchLabels:
      team: important-project
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-dev
          labels:
            group: {{ .Name }}
            namespace: important-project-dev
          annotations:
            my-annotation: justtext-dev
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-staging
          labels:
            group: {{ .Name }}
            namespace: important-project-staging
          annotations:
            my-annotation: justtext-staging
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-prod
          labels:
            group: {{ .Name }}
            namespace: important-project-prod
          annotations:
            my-annotation: justtext-prod
----

Und damit sind wir auch schon beim letzten Teil des Beispiels, der NamespaceConfig. Hier haben wir drei verschiedene NamespaceConfigs, die durch den Labelselector jeweils automatisch auf ein anderes Label reagieren, sobald ein entsprechender Namespace erstellt wird. Dadurch können wir für jeden unserer Namespaces eine eigene Konfiguration vorgeben.

[source,yaml]
----
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-dev
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-dev
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "4" 
            requests.cpu: "1" 
            requests.memory: 1Gi 
            limits.cpu: "2" 
            limits.memory: 2Gi 
---
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-staging
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-staging
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "8" 
            requests.cpu: "2"
            requests.memory: 2Gi 
            limits.cpu: "4" 
            limits.memory: 4Gi
---
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-prod
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-prod
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "8" 
            requests.cpu: "2" 
            requests.memory: 2Gi 
            limits.cpu: "4" 
            limits.memory: 4Gi
    - objectTemplate: |
        apiVersion: v1
        kind: LimitRange
        metadata:
          name: projectlimitrange
          namespace: {{ .Name }}
        spec:
          limits:
            - default:
                memory: 512Mi
              defaultRequest:
                memory: 256Mi
              type: Container
----

Es ist nebenbei egal, in welcher Reihenfolge diese Ressourcen erstellt werden.

Der Operator ist noch weit flexibler. Beispielsweise kann auch mit einer "MatchExpressions" auf Annotationen reagiert werden und mehr.

CAUTION: Der Hinweis bezüglich der Berechtigungen des GitOps-Serviceaccounts aus der zweiten Option gilt natürlich auch hier.

CAUTION: Einen Wermutstropfen hat auch diese Option: Der Operator ist nur als Communityoperator vorhanden, war aber trotzdem der Vorschlag von Red Hat für diese Thematik.