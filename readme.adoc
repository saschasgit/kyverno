= Kyverno - Eine Übersicht

:toc:

== Vorwort

Kyverno ist ein Admission Controller, welcher sehr viele Möglichkeiten gibt, um über Policies Vorgaben auf einem OpenShift-Cluster umzusetzen. Dabei ist Kyverno "Kubernetes-native" und verwendet als Sprache für die Polices YAML, was es für viele einfacher macht, als Polices in Rego zu schreiben.

== Admission Controller

Zunächst kurz ein paar Sätze dazu, was überhaupt "Admission Controller" sind. Immer, wenn ein Objekt in OpenShift angelegt, verändert oder gelöscht wird, läuft diese Anforderung über die API von OpenShift. Und das unabhängig davon, ob man den OC-Client oder die Weboberfläche verwendet.

Und während das Opject über die API läuft, gibt einem OpenShift die Möglichkeit, bei beliebigen Objekten einzugreifen. Die Objekte können verändert und überprüft werden.

https://cloud.redhat.com/blog/dynamic-admission-control

image:pictures/admission-controller.png["Admission Controller"]

Wie man sieht, gibt es eine "Mutating admission" und eine "Validating admission", über die man durch den Webhook eingreifen kann. Und genau diesen Mechanismus nutzen Admission Controller.

Auf einem OpenShift Cluster gibt es unterschiedliche Admission Controller. Ein Beispiel ist der Red Hat Service Mesh, wenn man diesen Operator verwendet. Die Control Plane des Service Meshs prüft beispielsweise über einen eigenen Admission Controller, ob eine bestimmte Annotation in einem Deployment vorhanden ist und fügt dann allen Pods einen Envoy-Proxy als zusätzlichen Container hinzu. Dabei wird die "Mutating admission" verwendet.

Ein Beispiel für die "Validating admission" sind RASP-Tools wie "Red Hat Advanced Cluster Security", "Aquasec" oder "Neuvector". Diese RASP-Tools wollen Objekte nicht verändern, aber überprüfen, ob diese vorgegebenen Anforderungen entsprechen, um die Sicherheit zu gewährleisten.

Kyverno kann beide ansprechen, sowohl "Mutating admission" als auch "Validating admission" und ist damit extrem flexibel einsetzbar.

== Die Kyverno Dokumentation

Die, sehr umfangreiche, Dokumentation zu Kyverno findet man unter https://kyverno.io/docs/

An dieser Stelle kann nur ein erster Überblick bzw. Einstieg gegeben werden. Kyverno ist viel zu umfangreich, um alles hier darzustellen.

== Die Installation von Kyverno

https://kyverno.io/docs/installation/methods/

Kyverno wird normalerweise über Helm installiert. Dies kann man direkt machen, so wie es unter obigem Link beschrieben ist. Bei Bedarf kann man sich den aktuellen Chart lokal herunterladen und verwenden. Dieses Beispiel lädt das Chart in das aktuelle Verzeichnis herunter:

[source]
----
helm pull kyverno/kyverno --untar=true --untardir=.
----

== Die wichtigsten CRDs von Kyverno

Kyverno bringt einige CRDs mit sich. Die wichtigsten sind:

Policy
Eine Policy, welche sich nur auf einen angegebenen Namespace bezieht.

ClusterPolicy
Eine clusterweite Policy

AdmissionReport
Eine Auswertung der Policyanwendungen auf Namespaceabene.

ClusterAdmissionReport
Eine Auswertung der Policyanwendungen auf Clusterebene.

BackgroundScanReport

ClusterBackgroundScanReport

== Admission ClusterPolicy "Check Namespace"

Fangen wir einfach mit einer ersten Policy einmal an. Wir wollen prüfen, ob ein neu ersteller Namespace ein bestimmtes Label enthält. 

Datei: 01-validate-clusterpolicy-check-namespace-label.yaml

[source,yaml]
----
apiVersion: kyverno.io/v2beta1
kind: ClusterPolicy
metadata:
  annotations:
    policies.kyverno.io/description: >-
      Checks if all namespaces have set a label "hello" with the value "world"
    policies.kyverno.io/severity: low
    policies.kyverno.io/subject: Namespace
    policies.kyverno.io/title: Test Namespace Annotation
  name: test-namespace-annotation
spec:
  background: false
  rules:
    - exclude:
        any:
          - resources:
              namespaces:
                - openshift*
                - kube*
                - default
      match:
        any:
          - resources:
              kinds:
                - Namespace
      name: test-namespace-label
      validate:
        anyPattern:
          - metadata:
              label:
                hello: world
        message: All new Namespaces must have a label "hello" with the value "world".
  validationFailureAction: audit
----

Die wichtigsten Elemente der Policy kurz erklärt:

Am "kind" sehen wir, dass es sich natürlich um eine ClusterPolicy handelt. Anders würde das prüfen von Namespaces auch keinen Sinn ergeben.

Im Abschnitt "rules" finden wir die verschiedenen Abschnitte "exclude", "match" und "validate".

Im Abschnitt "exclude" stehen dabei Regeln, auf die diese Policy nicht zutreffen soll. Wir möchten nicht, dass diese Regel auf Namespaces angewendet wird, die mit "openshift" oder "kube" beginnen und sie soll auch nicht für einen Namespace "default" gelten.

Im Abschnitt "match" befindet sich die Regel, worauf die Policy angewendet werden soll. Sie woll für alle Ressoucen vom Typ "Namespace" gelten.

Im Abschnitt "validate" steht, was genau geprüft werden soll. Dass es diesen Abschnitt gibt bedeutet, dass wir hier eine "Validate admission" haben. Geprüft wird, ob das angegebene Label vorhanden ist.

Am Ende steht noch "validationFailureAction: audit". Wir möchten zunächst nur, dass die Policy im "Audit" Modus läuft. Dann wird sie bereits verwendet, aber sie wird noch nicht erzwungen. Ein Namespace kann daher jetzt auch dann noch angelegt werden, wenn das Label nicht vorhanden ist. Die Alternative wäre "validationFailureAction: enforce".

Wenn diese Policy nun aktiv ist, erstellen wir einmal zwei Namespaces:

[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: a-test
  labels:
    hello: world
----

[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: b-test
----

Wenn man die Namespaces anlegt, dann passiert zunächst nichts Besonderes. Dadurch, dass wir eine ClusterPolicy haben, erstellt Kyverno nun aber automatisch einen ClusterAdmissionReport:

[source]
----
oc get clusteradmissionreport -A
NAME                                   AGE   PASS   FAIL   WARN   ERROR   SKIP
55a254a2-eee1-4d29-a6b7-5b1ba890b3be   7s    1      0      0      0       0
839c2835-c9bd-4f41-a7d3-87cd4d8466c1   5s    0      1      0      0       0
----

Und hier sehen wir, dass ein Namespace der Regel entsprach ("PASS") und einer nicht ("FAIL").

Wenn wir in einen Report schauen, sehen wir in der YAML, worauf sich der Report bezieht, also in unserem Fall einen bestimmten Namespace und entweder "result: fail" oder "result: pass".

An dieser Stelle ändern wir nun die Policy. Wir löschen nun einmal die beiden Namespaces und die Policy und verwenden diese neue Policy, die genau das gleiche macht, nun aber als Action nicht mehr "audit", sondern "enforce" hat:

Datei: 02-validate-clusterpolicy-check-namespace-label.yaml

[source,yaml]
----
apiVersion: kyverno.io/v2beta1
kind: ClusterPolicy
metadata:
  annotations:
    policies.kyverno.io/description: >-
      Checks if all namespaces have set a label "hello" with the value "world"
    policies.kyverno.io/severity: low
    policies.kyverno.io/subject: Namespace
    policies.kyverno.io/title: Enforce Namespace Annotation
  name: enforce-namespace-annotation
spec:
  background: false
  rules:
    - exclude:
        any:
          - resources:
              namespaces:
                - openshift*
                - kube*
                - default
      match:
        any:
          - resources:
              kinds:
                - Namespace
      name: test-namespace-label
      validate:
        anyPattern:
          - metadata:
              label:
                hello: world
        message: All new Namespaces must have a label "hello" with the value "world".
  validationFailureAction: enforce
----

Und jetzt sehen wir durchaus sofort die Auswirkung. Der erste Namespace lässt sich problemlos anlegen. Aber beim zweiten Namespace erhalten wir eine Fehlermeldung, welche auch die "message" unserer Policy enthält:

[source]
----
Error from server: error when creating ".\\01-namespace-b.yaml": admission webhook "validate.kyverno.svc-fail" denied the request:

resource Namespace//b-test was blocked due to the following policies

enforce-namespace-annotation:
  test-namespace-label: 'validation error: All new Namespaces must have a label "hello"
    with the value "world". rule test-namespace-label[0] failed at path /metadata/labels/hello/'
----

Und wenn wir uns dann die ClusterAdmissionReports ansehen, stellen wir fest, dass wir nur einen erhalten, nicht zwei:

[source]
----
oc get clusteradmissionreport -A
NAME                                   AGE   PASS   FAIL   WARN   ERROR   SKIP
4af50de4-22bd-48e8-97a9-c805559462ce   77s   1      0      0      0       0
----

Der Grund ist, dass im "Enforce" Modus keine Reports erstellt werden können, da die dazugehörige Ressource gar nicht existiert. Sie wurde ja geblockt.

== Ein Background Check

Datei: 03-validate-clusterpolicy-check-namespace-label.yaml

Nun löschen wir wieder unsere Policy und legen dann auch den zweiten Namespace einmal an, so dass wir beide Namespaces haben.

Dann

[source,yaml]
----
apiVersion: kyverno.io/v2beta1
kind: ClusterPolicy
metadata:
  annotations:
    policies.kyverno.io/description: >-
      Checks if all namespaces have set a label "hello" with the value "world"
    policies.kyverno.io/severity: low
    policies.kyverno.io/subject: Namespace
    policies.kyverno.io/title: Background Test Namespace Label
  name: background-test-namespace-label
spec:
  background: true
  rules:
    - exclude:
        any:
          - resources:
              namespaces:
                - openshift*
                - kube*
                - default
      match:
        any:
          - resources:
              kinds:
                - Namespace
      name: test-namespace-label
      validate:
        anyPattern:
          - metadata:
              labels:
                hello: world
        message: All new Namespaces must have a label "hello" with the value "world".
  validationFailureAction: audit
----

Wir haben nun "background: true" konfiguriert. Nun schaut Kyverno nicht nur beim Erstellen eines Objekts nach, sondern prüft einmal alle Objekte auf dem Cluster, die zu der Policy passen. Also werden nun alle Namespaces auf das Label geprüft, außer die Namespaces "openshift*", "kube*" und "default".

Das Ergebnis sehen wir mir wir folgt:

[source]
----
oc get clusterbackgroundscanreport -A
NAME                                   PASS   FAIL   WARN   ERROR   SKIP   AGE
064b357b-3fdd-48a5-9cbc-c5633781f94d   0      1      0      0       0      104s
4af50de4-22bd-48e8-97a9-c805559462ce   1      0      0      0       0      104s
88c10d5b-75cf-49b8-b2d6-3a1f51f910a3   0      1      0      0       0      104s
adc0ab33-37f2-4bb1-86f1-952c2103cf54   0      1      0      0       0      104s
----

Wir sehen, dass wir einen Namespace haben, der das Label besitzt, unseren Namespace "a-test".

Dann sehen wir, dass es drei Namespaces gibt, welche das Label nicht haben. Hier waren dies: b-test, hostpath-provisioner und kyverno.

Man sieht schon daran, dass man bei Policies, welche "validationFailureAction: enforce" verwenden, wirklich aufpassen muss, damit man nicht die Erstellung von Ressourcen verhindert, welche angelegt werden sollen oder gar müssen.

Den Namespace "kyverno" kann man übrigens durch entsprechende Konfiguration der "values.yaml" für Kyverno direkt von allen Policies ausnehmen.

Daher ist es eine gute Idee, wenn möglich, zunächst einen BackgroundScan zu aktivieren.

Backgroundscans arbeiten immer im Modus "Audit", auch wenn "Enforce" in der Policy konfiguriert ist. Es gibt aber einen Unterschied bei den Reports: https://main.kyverno.io/docs/policy-reports/background/

== Einschränkung der BackgroundScans

Die backgroundScans haben eine wichtige Einschränkung, die unten auf der Seite beschrieben ist: https://main.kyverno.io/docs/policy-reports/background/

Sie funktionieren nur bei Objekten aus den Bereichen "request.object" und "request.namespace". Daher funktionieren sie nicht bei z.B. Rollen.

Was aber hat es mit dem Bereich "request.object" überhaupt auf sich? Denn in einer Ressource wie "Namespace" finden wir den Bereich nicht. Das liegt daran, dass am Admission Controller nicht das Objekt "Namespace" ankommt, sondern ein "AdmissionReview" für ein Namespace-Objekt.

Informationen dazu findet man unter https://kyverno.io/docs/writing-policies/variables/

== Der AdmissionReview

Kyverno kann, im Gegensatz zu vielen anderen Tools, den AdmissionReview sichtbar machen. Diesen kann man nämlich nicht direkt über "oc" auslesen, es ist auch ein flüchtiges Objekt, welches nur kurzfristig existiert.

Das dafür notwendige Setting findet man auf der Seite https://kyverno.io/docs/troubleshooting/#policy-definition-not-working

Man sollte das nur kurzfristig machen und keinesfalls dauerhaft aktiv lassen, da es mehr Last erzeugt.

Wenn wir in die YAML des Deployments für den "kyverno-admission-Controller" schauen, findet man dort bereits die Einstellung "--dumpPayload=false". Diesen setzen wir nun einmal auf "true", woraufhin OpenShift die Pod neu startet.

Wenn wir nun einen neuen Namespace anlegen und dann die Logs für die Pod des "kyverno-admission-controllers" betrachten, sehen wir dort den AdmissionReview, der natürlich noch formatiert werden muss.

Und darin finden wir dann die entsprechenden Abschnitt des Requests wie "object" oder "namespace". Aber auch die, welche wir eben nicht mit einem BackgroundScan anfragen können wie "userInfo".

[source,json]
----
{
    "uid": "e467ec26-f070-4d3f-89ca-70cc143cacca",
    "kind": {
        "group": "",
        "version": "v1",
        "kind": "Namespace"
    },
    "resource": {
        "group": "",
        "version": "v1",
        "resource": "namespaces"
    },
    "requestKind": {
        "group": "",
        "version": "v1",
        "kind": "Namespace"
    },
    "requestResource": {
        "group": "",
        "version": "v1",
        "resource": "namespaces"
    },
    "name": "c-test",
    "namespace": "c-test",
    "operation": "CREATE",
    "userInfo": {
        "username": "kubeadmin",
        "uid": "935f70c6-fa2a-4864-aff6-db3120b71059",
        "groups": [
            "system:authenticated:oauth",
            "system:authenticated"
        ],
        "extra": {
            "scopes.authorization.openshift.io": [
                "user:full"
            ]
        }
    },
    "roles": [
        "kube-system:extension-apiserver-authentication-reader",
        "openshift-config-managed:console-public",
        "openshift-config-managed:openshift-network-public-role",
        "openshift-config-managed:system:openshift:oauth-servercert-trust",
        "openshift-console-user-settings:user-settings-935f70c6-fa2a-4864-aff6-db3120b71059-role",
        "openshift:copied-csv-viewer",
        "openshift:shared-resource-viewer"
    ],
    "clusterRoles": [
        "basic-user",
        "cluster-admin",
        "cluster-status",
        "console-extensions-reader",
        "helm-chartrepos-viewer",
        "self-access-reviewer",
        "self-provisioner",
        "system:basic-user",
        "system:build-strategy-docker",
        "system:build-strategy-jenkinspipeline",
        "system:build-strategy-source",
        "system:discovery",
        "system:oauth-token-deleter",
        "system:openshift:discovery",
        "system:openshift:public-info-viewer",
        "system:openshift:scc:restricted-v2",
        "system:openshift:useroauthaccesstoken-manager",
        "system:public-info-viewer",
        "system:scope-impersonation",
        "system:webhook"
    ],
    "object": {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {
            "creationTimestamp": "2023-06-18T15:04:30Z",
            "labels": {
                "kubernetes.io/metadata.name": "c-test"
            },
            "managedFields": [
                {
                    "apiVersion": "v1",
                    "fieldsType": "FieldsV1",
                    "fieldsV1": {
                        "f:metadata": {
                            "f:labels": {
                                ".": {},
                                "f:kubernetes.io/metadata.name": {}
                            }
                        }
                    },
                    "manager": "Mozilla",
                    "operation": "Update",
                    "time": "2023-06-18T15:04:30Z"
                }
            ],
            "name": "c-test",
            "uid": "8f537c89-e6b9-455b-b649-283cc91cc945"
        },
        "spec": {
            "finalizers": [
                "kubernetes"
            ]
        },
        "status": {
            "phase": "Active"
        }
    },
    "oldObject": null,
    "dryRun": false,
    "options": {
        "apiVersion": "meta.k8s.io/v1",
        "kind": "CreateOptions"
    }
}
----

== Mutation ClusterPolicy "Add Resources to BuildConfig"

Wie schon erwähnt, kann Kyverno nicht nur Admission Policies nutzen, sondern auch Mutating Policies. Davon gibt es mehrere Arten. Wir beginnen mit der üblichen Mutation Policy.

Dazu nehmen wir uns folgendes Szenario: Ein Cluster hat gesetzte Limitranges und dort Werte für Requets und Limits. Diese gelten, wenn nichts anderes vom Entwickler konfiguriert wird, für alle Pods, welche auf dem Cluster erstellt werden. Oft benötigen Pods gar nicht so viele Ressourcen, so dass der Wert in den Limitranges entsprechend gewählt ist. Daraus resultiert jedoch, dass Builds, wenn in den BuildConfigs nicht höhere Ressourcenwerte angegeben sind, recht langsam laufen. Wir wollen also erreichen, dass eine BuildConfig immer Ressourcenwerte für Limits besitzt. Entweder, weil der Entwickler welche angegeben hat oder durch unsere Policy.

Eine solche Policy kann wie folgt aussehen:

[source,yaml]
----
apiVersion : kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-resources-to-buildconfig
  annotations:
    policies.kyverno.io/title: Add Resources to BuildConfig
    policies.kyverno.io/category: Other
    policies.kyverno.io/severity: medium
    policies.kyverno.io/subject: BuildConfig
    policies.kyverno.io/description: >-
      If a BuildConfig does not specify at least resource limits it can happen
      that the build will be slow because the default-values of the limitranges settings
      will be applied.
      This policy checks if settings for memory and cpu limits are available and if not
      applies the settings below.
      This policy will exclude all namespaces that start with "openshift" or "kube".
      IMPORTANT: The setting "schemaValidation: false" is because of a bug in the actual
      kyverno version and can be deleted in 1.9+.
      IMPORTANT: If there is no nodeselector or the nodeselector is set to "null", than
      this version of kyverno shows in the annotation of the buildconfig "removed /spec/nodeSelector".
      This seems to be a bug in this version. The nodeselector will not be touched, only the resources.
spec:
  background: false
  schemaValidation: false
  rules:
  - name: add-resources-to-buildconfig
    match:
      any:
      - resources:
          kinds:
          - BuildConfig
    exclude:
      any:
      - resources:
          namespaces:
          - openshift*
          - kube*
    preconditions:
      any:
      - key: "{{request.operation || 'BACKGROUND'}}"
        operator: AnyIn
        value:
        - CREATE
        - UPDATE
    mutate:
      patchStrategicMerge:
        spec:
          resources:
            limits:
              +(memory): "2Gi"
              +(cpu): "1"
----

Die Policy schaut auf alle neu erstellten Ressourcen vom Typ "BuildConfig", wenn es sich um eine Neuerstellung oder ein Update der Ressource handelt. Sie ignoriert die Namespaces "openshift*" und "kube*".

Im Gegensatz zu einer Validation Policy hat diese Policy einen Anschnitt "mutate". Dort steht 

[source,yaml]
----
resources:
  limits:
    +(memory): "2Gi"
    +(cpu): "1"
----

Das "+" mit der Klammer bedeutet, dass diese Werte dann hinzugefügt werden, wenn diese noch nicht existieren. So kann der Entwickler trotzdem eigene Werte angeben. Kyverno kennt verschiedene solcher Methoden:

https://kyverno.io/docs/writing-policies/mutate/#conditional-logic-using-anchors
https://kyverno.io/docs/writing-policies/validate/#anchors

== Mutation ClusterPolicy "Add Creator"

Ein anderes Beispiel ist folgende Mutation Policy:

[source,yaml]
----
resour
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-creator
spec:
  background: false
  rules:
  - name: add-creator
    match:
      any:
      - resources:
          kinds:
          - Deployment
          - BuildConfig
          - Namespace
          - DeploymentConfig
          - StatefulSet
    mutate:
      patchStrategicMerge:
        metadata:
          annotations:
            created-by: "{{request.userInfo.username}}"
----


== Kyverno CLI

Kyverno besitzt auch ein Kommandozeilentool: Kyverno CLI: https://kyverno.io/docs/kyverno-cli/

Das jeweils aktuelle Release finden wir unter https://github.com/kyverno/kyverno/releases

Man muss unbedingt darauf achten, dass die CLI Version zur eingesetzten Kyverno Version passt. Bei der 3.0.9 von Kyverno ergaben sich da einige wesentliche Änderungen. Zur 3.0.0 passt die CLI Version 1.10.0:

[source]
----
DESKTOP-6ELUJDD:~$ kyverno version

Version: 1.10.0
Time: 2023-05-30T10:01:31Z
Git commit ID: da6f5c18132f773af15d0e09cbf2e16a36725232
----


















CAUTION: Das Problem lauert aber an einer anderen Stelle. Wenn wir nur eine GitOps-Instanz haben, dann haben wir auch nur einen dahinterliegenden GitOps-Serviceaccount mit den entsprechenden Berechtigungen in dem Namespace. Und wenn ein Developerteam GitOps verwendet, dann werden die Ressourcen nicht mit den Berechtigungen des Developers, sondern mit den Berechtigungen des GitOps-Serviceaccounts erstellt. Hat also der Clusteradmin die Möglichkeit, über GitOps eine Networkpolicy auszurollen, dann hat der Developer das in seinem Namespace ebenso.

Nun kann man dies noch über die Argo-Applikationsprojekte lösen, denn dort lässt sich konfigurieren, welche Art von Kubernetesobjekten wie z.B. "NetworkPolicy" erstellt werden dürfen und welche nicht. Ebenso lässt sich konfigurieren, in welchen Namespaces die Berechtigungen gelten. Diese Konfiguration muss aber sehr bedacht vorgenommen werden, damit am Ende die Berechtigungen korrekt sind.

Wenn man jedoch unterschiedliche Instanzen von Red Hat GitOps einsetzen möchte oder dies gar eine Securityanforderung ist, muss man anders vorgehen. Hierfür sind die beiden folgenden Optionen gedacht.

== Option 2: Getrennte Instanzen und Konfiguration der Instanzberechtigungen

Wenn man getrennte Instanzen von Red Hat GitOps einsetzt, stösst man auf ein Problem. Über ein Label "argocd.argoproj.io/managed-by:" wird in einem Namespace angegeben, welche Instanz diesen Namespace verwaltet. Daher hat erst einmal nur eine Instanz von Red Hat GitOps die Berechtigung, auf den Namespace zuzugreifen.

NOTE: Diese Option habe ich bisher nicht getestet, es dürfte aber keine technischen Probleme geben.

Wenn man nun beide Instanzen berechtigen möchte, auf einen Namespace zuzugreifen, muss man wissen, was hinter den Kulissen passiert, wenn ein Namespace ein entsprechendes Label erhält.

image:pictures/berechtigungsprozess3.png["Berechtigungsprozess"]

Wenn ein Namespace mit dem entsprechenden Label erstellt wird, bermerkt GitOps dies und erstellt im Normalfall eine Rolle und ein Rolebinding für den Serviceaccount des ArgoCD-Application-Controllers. Diese beiden Ressourcen sind an den Namespace gebunden, so dass GitOps darüber Zugriff auf den Namespace erhält.

Diese Rolle und das Rolebinding können wir natürlich auch manuell erstellen, also mit GitOps ausrollen, um die gleichen Berechtigungen wie durch das Label zu erhalten. So haben wir die Möglichkeit, dass die GitOps-Instanz für die Clusteradministratoren über diesen manuellen Weg und die GitOps-Instanz der Developer über das Label im Namespace Berechtigungen auf den Namespace erhalten. (Das geht natürlich grundsätzlich auch anders herum.)

image:pictures/instanzberechtigungen2.png["Instanzberechtigungen"]

CAUTION: Auch hier muss man aufpassen. Das Problem ist die GitOps-Instanz für die Developer. Denn der Serviceaccount, der für die GitOps-Instanz Ressourcen ausrollt, hat zunächst einmal die sehr weitgehende Rechte des Namespace-Admins und damit beispielsweise die Rechte zum Erstellen, Ändern oder Löschen von Subscriptions oder NetworkPolicies. Daraus folgt, dass man dafür sorgen muss, dass dass der Serviceaccount nicht diese weitgehenden Berechtigungen erhält.

Dieses Problem hat man auch im ArgoCD Projekt erkannt (vor ca. einem Jahr war das noch ein größeres Problem) und dafür eine Option erschaffen. Es gibt eine Umgebungsvariable "CONTROLLER_CLUSTER_ROLE", welche auf die Rolle verweist, welche GitOps für den Application-Controller verwendet. Diese kann in der Subscription gesetzt werden. Siehe dazu auch:

https://argocd-operator.readthedocs.io/en/latest/usage/custom_roles/

https://docs.openshift.com/container-platform/4.10/cicd/gitops/gitops-release-notes.html#new-features-1-4-0_gitops-release-notes

https://issues.redhat.com/browse/GITOPS-1290

IMPORTANT: Ich habe noch nicht getestet, ob es eine einfach Option gibt, dieses Verhalten je nach Instanz zu steuern. Das Setting bezieht sich auf die Subscription, also den Operator und die Instanzen erben dies vom Operator. Red Hat schreibt zwar (siehe Issue Link) "Cluster admin can choose to maintain the existing behaviour and request Argo CD to be namespace-admin", bisher ist mir aber noch nicht klar, wie dies einfach gesetzt werden kann.

== Option 3: Getrennte Instanzen und Einsatz des Namespace Configuration Operators

Eine Möglichkeit, dies eleganter zu lösen, so dass man am Ende gar nicht zwei verschiedenen GitOps-Instanzen Berechtigungen auf den Namespace geben muss, ist der Einsatz des "Namespace Configuration Operators". Mit diesem lassen sich sowohl die Namespaces als auch die gewünschte Konfiguration innerhalb dieser Namespaces konfigurieren und über GitOps ausrollen.

image:pictures/namespaceconfigurator.png["Namespace Configuration Operator"]

Der Operator bringt drei CRDs mit sich:

.CRDs
* NamespaceConfig
* GroupConfig
* UserConfig

Die Namen dieser drei CRDs beziehen sich nicht darauf, was sie konfigurieren, sondern durch welches Objekt sie getriggert werden, beispielsweise reagiert eine NamespaceConfig, wenn ein Namespace erstellt wird und nimmt dann eine Konfiguration vor. Und mit einer UserConfig kann man beispielsweise automatisch bei der Anlage eines Users diesem einen Namespace mit einer vorgegebenen Konfiguration bereitstellen.

An dieser Stelle wird nur eine einfache Konfiguration vorgenommen. Wer detaillierter sehen möchte, was der Operator für Möglichkeiten bietet, kann sich folgende Links ansehen:

https://github.com/redhat-cop/namespace-configuration-operator

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/namespace-config/readme.md

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/user-sandbox/readme.md

https://github.com/redhat-cop/namespace-configuration-operator/blob/master/examples/team-onboarding/group-config.yaml

In unserem Beispiel sollen bei der Anlage einer Gruppe (was natürlich über ArgoCD geschehen kann) drei Namespaces angelegt werden und verschiedene RessourceQuotas gesetzt werden. Bei einem Namespace werden zudem LimitRanges angelegt.

NOTE: Dies ist nur ein einfaches Beispiel. Man kann innerhalb der Namespaces beliebige Ressourcen erstellen, beispielsweise NetworkPolicies, etc.

Zunächst legen wir in dem Beispiel zwei Gruppen mit jeweils zwei Usern an. Jedoch wird nur eine der beiden Gruppen, die erste, mit einem Label versehen: "team: important-project"

[source,yaml]
----
kind: Group
apiVersion: user.openshift.io/v1
metadata:
  name: namespace-test-group
  labels:
    team: important-project
users:
  - namespace-testuser1
  - namespace-testuser2
---
kind: Group
apiVersion: user.openshift.io/v1
metadata:
  name: namespace-test-group2
users:
  - namespace-testuser3
  - namespace-testuser4 
----

Nun legen wir eine GroupConfig an. Diese hat einen Labelselector, der zum Label unser ersten Gruppe passt. Damit reagiert die GroupConfig nur auf die Erstellung unser ersten Gruppe, bei der zweiten bleibt der Operator unttätig.

Durch diese GroupConfig werden durch die Erstellung der ersten Gruppe automatisch drei Namespaces angelegt und noch eine beliebge Annotation hinzugefügt. Zudem erhält jeder Namespace ein eigenes Label mit seinem Namen: "namespace: important-project-dev", etc.

[source,yaml]
----
kind: GroupConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-groupconfig
spec:
  labelSelector:
    matchLabels:
      team: important-project
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-dev
          labels:
            group: {{ .Name }}
            namespace: important-project-dev
          annotations:
            my-annotation: justtext-dev
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-staging
          labels:
            group: {{ .Name }}
            namespace: important-project-staging
          annotations:
            my-annotation: justtext-staging
    - objectTemplate: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: important-project-prod
          labels:
            group: {{ .Name }}
            namespace: important-project-prod
          annotations:
            my-annotation: justtext-prod
----

Und damit sind wir auch schon beim letzten Teil des Beispiels, der NamespaceConfig. Hier haben wir drei verschiedene NamespaceConfigs, die durch den Labelselector jeweils automatisch auf ein anderes Label reagieren, sobald ein entsprechender Namespace erstellt wird. Dadurch können wir für jeden unserer Namespaces eine eigene Konfiguration vorgeben.

[source,yaml]
----
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-dev
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-dev
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "4" 
            requests.cpu: "1" 
            requests.memory: 1Gi 
            limits.cpu: "2" 
            limits.memory: 2Gi 
---
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-staging
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-staging
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "8" 
            requests.cpu: "2"
            requests.memory: 2Gi 
            limits.cpu: "4" 
            limits.memory: 4Gi
---
kind: NamespaceConfig
apiVersion: redhatcop.redhat.io/v1alpha1
metadata:
  name: test-namespaceconfig-prod
spec:
  labelSelector:
    matchLabels:
      namespace: important-project-prod
  templates:
    - objectTemplate: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: projectdefault
          namespace: {{ .Name }}
        spec:
          hard:
            pods: "8" 
            requests.cpu: "2" 
            requests.memory: 2Gi 
            limits.cpu: "4" 
            limits.memory: 4Gi
    - objectTemplate: |
        apiVersion: v1
        kind: LimitRange
        metadata:
          name: projectlimitrange
          namespace: {{ .Name }}
        spec:
          limits:
            - default:
                memory: 512Mi
              defaultRequest:
                memory: 256Mi
              type: Container
----

Es ist nebenbei egal, in welcher Reihenfolge diese Ressourcen erstellt werden.

Der Operator ist noch weit flexibler. Beispielsweise kann auch mit einer "MatchExpressions" auf Annotationen reagiert werden und mehr.

CAUTION: Der Hinweis bezüglich der Berechtigungen des GitOps-Serviceaccounts aus der zweiten Option gilt natürlich auch hier.

CAUTION: Einen Wermutstropfen hat auch diese Option: Der Operator ist nur als Communityoperator vorhanden, war aber trotzdem der Vorschlag von Red Hat für diese Thematik.